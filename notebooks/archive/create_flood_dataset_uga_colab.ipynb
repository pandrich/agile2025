{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2184c4d8",
   "metadata": {},
   "source": [
    "# Uganda Flood Timeseries — Colab-ready notebook\n",
    "\n",
    "This Colab notebook computes **per-Sentinel-1 acquisition × district** flood metrics and exports results as CSV files to your Google Drive. It runs the on-the-fly per-district monthly baseline and uses `Export.table.toDrive` for server-side exports.\n",
    "\n",
    "**Before you run:** upload your Uganda Admin-2 shapefile (all companion files .shp/.shx/.dbf/.prj) to a Drive folder and set `DRIVE_SHAPE_FOLDER` accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q earthengine-api geemap geopandas pyogrio rasterio pandas shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Authenticate and initialize Earth Engine and mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import ee, geemap\n",
    "ee.Authenticate()  # follow the link, paste token\n",
    "ee.Initialize()\n",
    "\n",
    "print('EE initialized.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cca77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- User parameters (edit) ----------\n",
    "# Path in Drive where your shapefile (uganda_admin2.*) lives, e.g. '/content/drive/MyDrive/uganda_shp'\n",
    "DRIVE_SHAPE_FOLDER = '/content/drive/MyDrive/uganda_shp'\n",
    "\n",
    "# Filenames inside that folder\n",
    "SHP_FILENAME = 'uganda_admin2.shp'  # make sure companion .dbf .shx are present\n",
    "\n",
    "# Baseline period and study period\n",
    "BASELINE_START = '2017-01-01'\n",
    "BASELINE_END   = '2019-12-31'\n",
    "STUDY_START = '2020-01-01'\n",
    "STUDY_END   = '2025-08-31'\n",
    "\n",
    "# Flood detection params\n",
    "ANOMALY_THRESHOLD = -3.0   # dB\n",
    "PERM_WATER_PCT = 50       # JRC occurrence threshold to treat as permanent water\n",
    "\n",
    "# Export settings: Drive folder to save CSVs\n",
    "EXPORT_FOLDER = 'EE_Flood_Exports'  # created inside your Drive MyDrive root\n",
    "IMAGES_PER_BATCH = 25  # how many S1 images to create export tasks for in one run (tune lower if tasks fail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70285d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load shapefile from Drive into geopandas and convert to EE FeatureCollection\n",
    "import geopandas as gpd, os\n",
    "from shapely.geometry import mapping\n",
    "shp_path = os.path.join(DRIVE_SHAPE_FOLDER, SHP_FILENAME)\n",
    "gdf = gpd.read_file(shp_path)\n",
    "gdf = gdf.to_crs(epsg=4326)\n",
    "# compute area in m2 and store as column for stability\n",
    "gdf['area_m2'] = gdf.geometry.to_crs(epsg=3857).area\n",
    "# choose a name field\n",
    "name_field = None\n",
    "for c in ['NAME_2','district','ADM2_NAME','name','DN']:\n",
    "    if c in gdf.columns:\n",
    "        name_field = c\n",
    "        break\n",
    "if name_field is None:\n",
    "    name_field = gdf.columns[0]\n",
    "\n",
    "# Convert to EE FeatureCollection and set area_m2 as property\n",
    "import geemap, ee\n",
    "fc = geemap.geopandas_to_ee(gdf, geodesic=False)\n",
    "# ensure area_m2 present as property (server-side)\n",
    "def set_area(feature):\n",
    "    return feature.set({'area_m2': ee.Number(feature.geometry().area())})\n",
    "fc = fc.map(set_area)\n",
    "\n",
    "print('Loaded shapefile with', len(gdf), 'features. Name field:', name_field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load collections and helper S1 filter\n",
    "S1_ID = 'COPERNICUS/S1_GRD'\n",
    "GSW_ID = 'JRC/GSW1_3/GlobalSurfaceWater'\n",
    "s1_col = ee.ImageCollection(S1_ID).filterDate(STUDY_START, STUDY_END)     .filter(ee.Filter.eq('instrumentMode', 'IW'))     .filter(ee.Filter.listContains('transmitterReceiverPolarisation','VV'))     .filterBounds(fc.geometry())\n",
    "\n",
    "gsw = ee.Image(GSW_ID).select('occurrence')\n",
    "print('Sentinel-1 images over study period (server-side collection size):', s1_col.size().getInfo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184db29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtain list of image IDs and times (client-side listing). For large numbers this may still be OK.\n",
    "# If too large, consider chunking by year and repeating this listing per year.\n",
    "info = s1_col.reduceColumns(ee.Reducer.toList(2), ['system:index','system:time_start']).getInfo()\n",
    "ids = info['list'][0]\n",
    "times = info['list'][1]\n",
    "print('Total S1 images found:', len(ids))\n",
    "# Build list of dicts for images\n",
    "images = [{'id': i, 'time_ms': t} for i,t in zip(ids,times)]\n",
    "# Optionally limit for a demo run:\n",
    "# images = images[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79794b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to process one S1 image and return a FeatureCollection of per-district results\n",
    "def process_image_to_fc(image_id):\n",
    "    img = ee.Image(S1_ID + '/' + image_id).select('VV')\n",
    "    img_date = ee.Date(img.get('system:time_start'))\n",
    "    month = img_date.get('month')  # 1..12\n",
    "    # baseline: images in baseline range and same calendar month, clipped to feature when used\n",
    "    def per_feature_fn(feature):\n",
    "        geom = feature.geometry()\n",
    "        # baseline collection for this district/month\n",
    "        baseline_col = ee.ImageCollection(S1_ID)             .filterDate(BASELINE_START, BASELINE_END)             .filter(ee.Filter.eq('instrumentMode','IW'))             .filter(ee.Filter.listContains('transmitterReceiverPolarisation','VV'))             .filterBounds(geom)             .filter(ee.Filter.calendarRange(month, month, 'month'))             .select('VV')\n",
    "        baseline = baseline_col.median().clip(geom)\n",
    "        baseline_exists = baseline_col.size().gt(0)\n",
    "        # clipped image\n",
    "        img_clip = img.clip(geom)\n",
    "        # anomaly and flood mask\n",
    "        anomaly = img_clip.subtract(baseline)\n",
    "        flood_mask = anomaly.lte(ANOMALY_THRESHOLD).And(gsw.lt(PERM_WATER_PCT))\n",
    "        # flooded area\n",
    "        flooded_area = ee.Image.pixelArea().updateMask(flood_mask).reduceRegion(\n",
    "            ee.Reducer.sum(), geom, scale=10, maxPixels=1e13\n",
    "        ).get('area')\n",
    "        flooded_area = ee.Number(flooded_area).unmask(0)\n",
    "        # district area from property (we set earlier)\n",
    "        district_area = ee.Number(feature.get('area_m2'))\n",
    "        flood_fraction = ee.Algorithms.If(district_area.gt(0), flooded_area.divide(district_area), None)\n",
    "        # coverage: fraction of district with any unmasked pixels in the image\n",
    "        valid_mask = img_clip.mask().gt(0)\n",
    "        valid_area = ee.Image.pixelArea().updateMask(valid_mask).reduceRegion(\n",
    "            ee.Reducer.sum(), geom, scale=10, maxPixels=1e13\n",
    "        ).get('area')\n",
    "        valid_area = ee.Number(valid_area).unmask(0)\n",
    "        coverage_pct = ee.Number(100).multiply(valid_area.divide(district_area))\n",
    "        # assemble feature properties\n",
    "        out = feature.set({\n",
    "            'image_id': image_id,\n",
    "            'acq_time': img_date.format('YYYY-MM-dd'T'HH:mm:ss'Z''),\n",
    "            'baseline_exists': baseline_exists,\n",
    "            'flooded_m2': flooded_area,\n",
    "            'district_area_m2': district_area,\n",
    "            'flood_fraction': flood_fraction,\n",
    "            'coverage_pct': coverage_pct\n",
    "        })\n",
    "        return out\n",
    "    return fc.map(per_feature_fn)\n",
    "\n",
    "# Export a single image's per-district results to Drive as CSV\n",
    "def export_image_to_drive(image_id, file_prefix):\n",
    "    results_fc = process_image_to_fc(image_id)\n",
    "    task = ee.batch.Export.table.toDrive({\n",
    "        'collection': results_fc,\n",
    "        'description': f'export_{image_id}',\n",
    "        'folder': EXPORT_FOLDER,\n",
    "        'fileNamePrefix': f'{file_prefix}_{image_id}',\n",
    "        'fileFormat': 'CSV'\n",
    "    })\n",
    "    task.start()\n",
    "    return task\n",
    "\n",
    "# Example: run export for first N images in batches to avoid overwhelming tasks\n",
    "from time import sleep\n",
    "n_images = len(images)\n",
    "print('Scheduling exports for', n_images, 'images (in batches of', IMAGES_PER_BATCH, ')')\n",
    "\n",
    "for i in range(0, n_images, IMAGES_PER_BATCH):\n",
    "    batch = images[i:i+IMAGES_PER_BATCH]\n",
    "    print(f'Batch {i//IMAGES_PER_BATCH + 1}: scheduling {len(batch)} exports...')\n",
    "    for item in batch:\n",
    "        imgid = item['id']\n",
    "        task = export_image_to_drive(imgid, 's1_districts')\n",
    "        print('Started task for image', imgid, 'task id:', task.id)\n",
    "        # optional: small sleep to pace task submissions\n",
    "        sleep(1)\n",
    "\n",
    "print('All export tasks submitted. Check the Earth Engine Tasks tab or drive folder to monitor exports.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35a5c4",
   "metadata": {},
   "source": [
    "\n",
    "## Notes after running\n",
    "\n",
    "- After tasks complete, you will find multiple CSV files in your Drive `EXPORT_FOLDER`. You can download and concatenate them locally to create the master CSV.\n",
    "\n",
    "- To concatenate in Colab:\n",
    "```python\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "files = glob('/content/drive/MyDrive/EE_Flood_Exports/*.csv')\n",
    "all = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "all.to_csv('/content/drive/MyDrive/uganda_flood_master.csv', index=False)\n",
    "```\n",
    "\n",
    "- If you want a preview run, limit `images = images[:5]` above and `IMAGES_PER_BATCH=2` to test.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
